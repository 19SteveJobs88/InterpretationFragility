# InterpretationFragility
Code for implementation of [Interpretation of Nueral Network is Fragile.](https://arxiv.org/pdf/1710.10547.pdf).

**Please cite the following work if you use this benchmark or the provided tools or implementations:**

    [1] Ghorbani, Amirata, Abubakar Abid, and James Zou. 
    "Interpretation of neural networks is fragile." 
    Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.
    
## The large scale results of attack methods against four famous feature-attribution methods
![alt text](https://github.com/amiratag/InterpretationFragility/blob/master/pictures/SaliencyMethodsComparison.png)

## Examples of targeted attack for semantically meaningful change in feature-importance
![alt text](https://github.com/amiratag/InterpretationFragility/blob/master/pictures/SemanticChange.png)

## Attack examples on Deep Taylor Decomposition
![alt text](https://github.com/amiratag/InterpretationFragility/blob/master/pictures/DTD_examples.png)
